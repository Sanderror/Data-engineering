{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52627d51",
   "metadata": {},
   "source": [
    "### Make sure to first upload the phishing.csv file to your data bucket in cloud storage to be able to run this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7094683",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp>2 in ./.local/lib/python3.10/site-packages (2.7.0)\n",
      "Collecting kfp>2\n",
      "  Downloading kfp-2.9.0.tar.gz (595 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m595.6/595.6 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-pipeline-components>2 in ./.local/lib/python3.10/site-packages (2.17.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.70.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in ./.local/lib/python3.10/site-packages (from kfp>2) (2.21.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.35.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.14.0)\n",
      "Collecting kfp-pipeline-spec==0.4.0 (from kfp>2)\n",
      "  Downloading kfp_pipeline_spec-0.4.0-py3-none-any.whl.metadata (301 bytes)\n",
      "Collecting kfp-server-api<2.4.0,>=2.1.0 (from kfp>2)\n",
      "  Downloading kfp_server_api-2.3.0.tar.gz (84 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: kubernetes<31,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (26.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in ./.local/lib/python3.10/site-packages (from kfp>2) (4.25.5)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (1.26.20)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components>2) (3.1.4)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in ./.local/lib/python3.10/site-packages (from kfp>2) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp>2) (2.0.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.24.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.5)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.9.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp>2) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp>2) (1.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components>2) (3.0.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp>2) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp>2) (2024.8.30)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (74.1.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<31,>=8.0.0->kfp>2) (2.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp>2) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<31,>=8.0.0->kfp>2) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7784286f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581f8117",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.7.0\n",
      "google-cloud-aiplatform==1.70.0\n",
      "google_cloud_pipeline_components version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b608be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,ModelDeployOp)\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331825b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"tranquil-lore-435419-m2\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://temp_de2024_2083033\"   # e.g., gs://temp_de2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724f172f-5145-4ce5-b73d-e399a2ca1de9",
   "metadata": {},
   "source": [
    "## First load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e762eac5-b00d-42a1-8abd-d515b8d670c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\",\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def load_data(project_id: str, bucket: str, filename: str, dataset: Output[Dataset]):\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the phishing dataset file from the google storage data bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob = bucket.blob(filename)\n",
    "    blob.download_to_filename(dataset.path + \".csv\")\n",
    "    logging.info('Downloaded Data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24944690-31c1-476a-b953-d799880bdc40",
   "metadata": {},
   "source": [
    "## Then split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8480d76-7cb0-40a0-9ef9-5a3658baeb67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn==1.3.2\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_test_split(dataset: Input[Dataset], dataset_train: Output[Dataset], dataset_test: Output[Dataset]):\n",
    "    '''train_test_split'''\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    from sklearn.model_selection import train_test_split as tts\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO) \n",
    "    \n",
    "    # Split the phishing dataset into a train set (80%) and test set (20%) and store those datasets\n",
    "    alldata = pd.read_csv(dataset.path+\".csv\", index_col=None)\n",
    "    train, test = tts(alldata, test_size=0.2, random_state=42)\n",
    "    train.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n",
    "    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af121d",
   "metadata": {},
   "source": [
    "## Then create a function that trains a SVM model on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc9fec7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_svm(train_set: Input[Dataset], test_set: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a SVM with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn import svm\n",
    "    from sklearn import metrics\n",
    "    from sklearn.metrics import recall_score, accuracy_score\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Load the train and test sets into dataframes\n",
    "    df_train = pd.read_csv(train_set.path+\".csv\")\n",
    "    df_test = pd.read_csv(test_set.path+\".csv\")\n",
    "\n",
    "    logging.info(df_train.columns)\n",
    "    logging.info(df_test.columns)  \n",
    "    \n",
    "    # Split the train and test sets into the X variables and the y variable\n",
    "    x_train, y_train = df_train.drop('CLASS_LABEL', axis=1), df_train['CLASS_LABEL']\n",
    "    x_test, y_test = df_test.drop('CLASS_LABEL', axis=1), df_test['CLASS_LABEL']\n",
    "    \n",
    "    # Train a SVM Classifier on the train data and predict the test data\n",
    "    model_svm = svm.SVC(random_state=42)\n",
    "    model_svm.fit(x_train,y_train)\n",
    "    y_pred = model_svm.predict(x_test)\n",
    "    \n",
    "    # Store the accuracy and recall of the model on the test set in a dictionary\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred)\n",
    "    }\n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    # Store some metadata\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"SVM\"\n",
    "    \n",
    "    # Save the model to a pickle file\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(model_svm, f)   \n",
    "    \n",
    "    # Return the metrics dictionary as an output\n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e104459",
   "metadata": {},
   "source": [
    "## Then train a Random Forest Classifier on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31dcb7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_rf(train_set: Input[Dataset], test_set: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a Random Forest with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import recall_score, accuracy_score\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Load the train and test sets into dataframes\n",
    "    df_train = pd.read_csv(train_set.path+\".csv\")\n",
    "    df_test = pd.read_csv(test_set.path+\".csv\")\n",
    "\n",
    "    logging.info(df_train.columns)\n",
    "    logging.info(df_test.columns)  \n",
    "    \n",
    "    # Split the train and test sets into the X variables and the y variable\n",
    "    x_train, y_train = df_train.drop('CLASS_LABEL', axis=1), df_train['CLASS_LABEL']\n",
    "    x_test, y_test = df_test.drop('CLASS_LABEL', axis=1), df_test['CLASS_LABEL']\n",
    "    \n",
    "    # Train a RF Classifier on the train data and predict the test data\n",
    "    model_rf = RandomForestClassifier(random_state=42) \n",
    "    model_rf.fit(x_train,y_train)\n",
    "    y_pred = model_rf.predict(x_test)\n",
    "\n",
    "    # Store the accuracy and recall of the model on the test set in a dictionary\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred)\n",
    "    }\n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    # Store some metadata\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"RF\"\n",
    "\n",
    "    # Save the model to a pickle file\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(model_rf, f)   \n",
    "\n",
    "    # Return the metrics dictionary as an output\n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccc549",
   "metadata": {},
   "source": [
    "## Then train a Logistic Regression on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8a80c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_lr(train_set: Input[Dataset], test_set: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a Logistic Regression with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import recall_score, accuracy_score\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "    # Load the train and test sets into dataframes\n",
    "    df_train = pd.read_csv(train_set.path+\".csv\")\n",
    "    df_test = pd.read_csv(test_set.path+\".csv\")\n",
    "\n",
    "    logging.info(df_train.columns)\n",
    "    logging.info(df_test.columns)  \n",
    "    \n",
    "    # Split the train and test sets into the X variables and the y variable\n",
    "    x_train, y_train = df_train.drop('CLASS_LABEL', axis=1), df_train['CLASS_LABEL']\n",
    "    x_test, y_test = df_test.drop('CLASS_LABEL', axis=1), df_test['CLASS_LABEL']\n",
    "\n",
    "    # Train a Logistic Regression on the train data and predict the test data    \n",
    "    model_lr = LogisticRegression(random_state=42)\n",
    "    model_lr.fit(x_train,y_train)\n",
    "    y_pred = model_lr.predict(x_test)\n",
    "\n",
    "    # Store the accuracy and recall of the model on the test set in a dictionary\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"recall\": recall_score(y_test, y_pred)\n",
    "    }\n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    # Store some metadata\n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"LR\"\n",
    "\n",
    "    # Save the model to a pickle file\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(model_lr, f)   \n",
    "\n",
    "    # Return the metrics dictionary as an output\n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdad3f",
   "metadata": {},
   "source": [
    "## Then we compare the two model's performances to decide which one to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35da0ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def compare_model(svm_metrics: dict, rf_metrics: dict, lr_metrics: dict) -> str:\n",
    "    '''Compare the SVM, RF, and LR model's performances on the test set with each other\n",
    "    and return the best model on recall (with at least an accuracy of 0.7). We want to optimize\n",
    "    recall, because the higher the recall, the minimal the type II error. The type II error in our\n",
    "    case means predicting that it is not a phishing link when in reality it is. This is a bigger \n",
    "    problem than predicting that it is a phishing link, when in reality it is not (better safe than sorry).\n",
    "    '''\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(svm_metrics)\n",
    "    logging.info(rf_metrics)\n",
    "    logging.info(lr_metrics)\n",
    "\n",
    "    # Obtain the recall and accuracy for the 3 models\n",
    "    svm_recall = svm_metrics.get('recall')\n",
    "    svm_accuracy = svm_metrics.get('accuracy')\n",
    "    \n",
    "    rf_recall = rf_metrics.get('recall')\n",
    "    rf_accuracy = rf_metrics.get('accuracy')\n",
    "    \n",
    "    lr_recall = lr_metrics.get('recall')\n",
    "    lr_accuracy = lr_metrics.get('accuracy')\n",
    "    \n",
    "    # We iteratively update the best_recall score and in this way can find the best model\n",
    "    best_model = None\n",
    "    best_recall = 0\n",
    "    \n",
    "    # Compare SVM model\n",
    "    if svm_accuracy >= 0.7 and svm_recall > best_recall:\n",
    "        best_model = 'SVM'\n",
    "        best_recall = svm_recall\n",
    "    \n",
    "    # Compare Random Forest model\n",
    "    if rf_accuracy >= 0.7 and rf_recall > best_recall:\n",
    "        best_model = 'RF'\n",
    "        best_recall = rf_recall\n",
    "    \n",
    "    # Compare Logistic Regression model\n",
    "    if lr_accuracy >= 0.7 and lr_recall > best_recall:\n",
    "        best_model = 'LR'\n",
    "        best_recall = lr_recall\n",
    "\n",
    "    # It could be the case that none of the models had an accuracy higher than 0.7\n",
    "    # In that case, we want to use the model with the highest accuracy\n",
    "    if best_model is None:\n",
    "        if svm_accuracy > lr_accuracy:\n",
    "            if svm_accuracy > rf_accuracy:\n",
    "                best_model = \"SVM\"\n",
    "            else:\n",
    "                best_model = \"RF\"\n",
    "        else:\n",
    "            if lr_accuracy > rf_accuracy:\n",
    "                best_model = \"LR\"\n",
    "            else:\n",
    "                best_model = \"RF\"\n",
    "            \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2dc14",
   "metadata": {},
   "source": [
    "## Then we upload the model to google cloud storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "394054fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model], model_name: str):\n",
    "    '''Upload the best model to Google Cloud Storage models bucket'''\n",
    "    from google.cloud import storage   \n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "    # Upload the model to GCS, and store it as model-A1.pkl\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(model_repo)\n",
    "    blob = bucket.blob('model-A1.pkl')\n",
    "    source_file_name= model.path + '.pkl'\n",
    "   \n",
    "    blob.upload_from_filename(source_file_name)    \n",
    "    \n",
    "    print(f\"File {source_file_name} uploaded to {model_repo}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d3ba3",
   "metadata": {},
   "source": [
    "## Then we create the complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41f2531c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_6263/2814439070.py:41: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with kfp.dsl.Condition(comp_model_op.output == 'SVM'):\n",
      "/var/tmp/ipykernel_6263/2814439070.py:50: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with kfp.dsl.Condition(comp_model_op.output == 'RF'):\n",
      "/var/tmp/ipykernel_6263/2814439070.py:59: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with kfp.dsl.Condition(comp_model_op.output == 'LR'):\n"
     ]
    }
   ],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"phishing-predictor-training-pipeline-final\",\n",
    "    description=\"This pipeline downloads the phishing dataset, trains 3 models, compares those models,\\\n",
    "        chooses the best model, and uploads that model to the model bucket in cloud storage\")\n",
    "def pipeline(project_id: str, data_bucket: str, filename:str, model_repo: str, model_repo_uri: str):\n",
    "    \n",
    "    # load the dataset\n",
    "    dataset_op = load_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        filename=filename)\n",
    "\n",
    "    # Split the dataset into a train (80%) and test (20%) set\n",
    "    train_test_split_op = train_test_split(dataset=dataset_op.outputs[\"dataset\"]).after(dataset_op)\n",
    "    \n",
    "    # Train and test a SVM Classifier on the data\n",
    "    training_svm_job_run_op = train_svm(\n",
    "        train_set=train_test_split_op.outputs[\"dataset_train\"],\n",
    "        test_set=train_test_split_op.outputs[\"dataset_test\"]\n",
    "    ).after(train_test_split_op)\n",
    "    \n",
    "    # Train and test a RF Classifier on the data\n",
    "    training_rf_job_run_op = train_rf(\n",
    "        train_set=train_test_split_op.outputs[\"dataset_train\"],\n",
    "        test_set=train_test_split_op.outputs[\"dataset_test\"]\n",
    "    ).after(train_test_split_op)\n",
    "    \n",
    "    # Train and test a Logistic Regression on the data\n",
    "    training_lr_job_run_op = train_lr(\n",
    "        train_set=train_test_split_op.outputs[\"dataset_train\"],\n",
    "        test_set=train_test_split_op.outputs[\"dataset_test\"]\n",
    "    ).after(train_test_split_op)\n",
    "        \n",
    "    # Compare the models based on their performance metrics and choose the best model\n",
    "    comp_model_op = compare_model(svm_metrics=training_svm_job_run_op.outputs[\"metrics\"],\n",
    "                                    rf_metrics=training_rf_job_run_op.outputs[\"metrics\"],\n",
    "                                    lr_metrics=training_lr_job_run_op.outputs[\"metrics\"]).after(training_svm_job_run_op, training_rf_job_run_op, training_lr_job_run_op)  \n",
    "    \n",
    "    # Conditional deployment based on the best-performing model\n",
    "    with kfp.dsl.Condition(comp_model_op.output == 'SVM'):\n",
    "        # If SVM is the best, upload the SVM model to GCS\n",
    "        upload_model_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_svm_job_run_op.outputs['out_model'],\n",
    "            model_name=\"SVM\"\n",
    "        ).after(comp_model_op)\n",
    "    \n",
    "    with kfp.dsl.Condition(comp_model_op.output == 'RF'):\n",
    "        # If Random Forest is the best, upload the RF model to GCS\n",
    "        upload_model_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_rf_job_run_op.outputs['out_model'],\n",
    "            model_name=\"RF\"\n",
    "        ).after(comp_model_op)\n",
    "\n",
    "    with kfp.dsl.Condition(comp_model_op.output == 'LR'):\n",
    "        # If Logistic Regression is the best, upload the LR model to GCS\n",
    "        upload_model_to_gc_op = upload_model_to_gcs(\n",
    "            project_id=project_id,\n",
    "            model_repo=model_repo,\n",
    "            model=training_lr_job_run_op.outputs['out_model'],\n",
    "            model_name=\"LR\"\n",
    "        ).after(comp_model_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb72f8",
   "metadata": {},
   "source": [
    "## Then we compile the YAML file, which is used to run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "258f5a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='phishing_predictor_training_pipeline_final.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d4b95",
   "metadata": {},
   "source": [
    "## Then we run the pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "105cc7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/454935101788/locations/us-central1/pipelineJobs/phishing-predictor-training-pipeline-final-20241024131622\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/454935101788/locations/us-central1/pipelineJobs/phishing-predictor-training-pipeline-final-20241024131622')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/phishing-predictor-training-pipeline-final-20241024131622?project=454935101788\n",
      "PipelineJob projects/454935101788/locations/us-central1/pipelineJobs/phishing-predictor-training-pipeline-final-20241024131622 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/454935101788/locations/us-central1/pipelineJobs/phishing-predictor-training-pipeline-final-20241024131622 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/454935101788/locations/us-central1/pipelineJobs/phishing-predictor-training-pipeline-final-20241024131622 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/454935101788/locations/us-central1/pipelineJobs/phishing-predictor-training-pipeline-final-20241024131622 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/454935101788/locations/us-central1/pipelineJobs/phishing-predictor-training-pipeline-final-20241024131622 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/454935101788/locations/us-central1/pipelineJobs/phishing-predictor-training-pipeline-final-20241024131622 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/454935101788/locations/us-central1/pipelineJobs/phishing-predictor-training-pipeline-final-20241024131622\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"phishing-predictor-final\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"phishing_predictor_training_pipeline_final.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID, # makesure to use your project id \n",
    "        'data_bucket': 'data_de2024_2083033',  # makesure to use your data bucket name \n",
    "        'filename': 'phishing.csv',\n",
    "        'model_repo':'models_de2024_2083033', # makesure to use your model bucket name \n",
    "        'model_repo_uri':'gs://models_de2024_2083033' # makesure to use your model bucket name \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e443112c",
   "metadata": {},
   "source": [
    "## Small test to see if model is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f5392",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"phishing-prediction-model-final\"\n",
    "! gcloud ai models list --region={REGION} --filter={DISPLAY_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666f27eb-8fc1-4619-9c9a-6a6e7f95bf07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d9253e3-7327-4a98-b6f7-341e0594008c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator SVC from version 1.3.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff88a1-a17e-4115-a483-c85d8ccc41a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
