# PIPELINE DEFINITION
# Name: phishing-predictor-training-pipeline-v3
# Inputs:
#    data_bucket: str
#    filename: str
#    model_repo: str
#    model_repo_uri: str
#    project_id: str
components:
  comp-compare-model:
    executorLabel: exec-compare-model
    inputDefinitions:
      parameters:
        lr_metrics:
          parameterType: STRUCT
        rf_metrics:
          parameterType: STRUCT
        svm_metrics:
          parameterType: STRUCT
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
  comp-condition-1:
    dag:
      tasks:
        importer:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-importer
          dependentTasks:
          - upload-model-to-gcs
          inputs:
            parameters:
              uri:
                componentInputParameter: pipelinechannel--model_repo_uri
          taskInfo:
            name: importer
        model-upload:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-model-upload
          dependentTasks:
          - importer
          inputs:
            artifacts:
              unmanaged_container_model:
                taskOutputArtifact:
                  outputArtifactKey: artifact
                  producerTask: importer
            parameters:
              display_name:
                runtimeValue:
                  constant: phishing-prediction-model-v3-SVM
              project:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: model-upload
        upload-model-to-gcs:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-model-to-gcs
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-svm-out_model
            parameters:
              model_name:
                runtimeValue:
                  constant: SVM
              model_repo:
                componentInputParameter: pipelinechannel--model_repo
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: upload-model-to-gcs
    inputDefinitions:
      artifacts:
        pipelinechannel--train-svm-out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--compare-model-Output:
          parameterType: STRING
        pipelinechannel--model_repo:
          parameterType: STRING
        pipelinechannel--model_repo_uri:
          parameterType: STRING
        pipelinechannel--project_id:
          parameterType: STRING
  comp-condition-2:
    dag:
      tasks:
        importer-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-importer-2
          dependentTasks:
          - upload-model-to-gcs-2
          inputs:
            parameters:
              uri:
                componentInputParameter: pipelinechannel--model_repo_uri
          taskInfo:
            name: importer-2
        model-upload-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-model-upload-2
          dependentTasks:
          - importer-2
          inputs:
            artifacts:
              unmanaged_container_model:
                taskOutputArtifact:
                  outputArtifactKey: artifact
                  producerTask: importer-2
            parameters:
              display_name:
                runtimeValue:
                  constant: phishing-prediction-model-v3-RF
              project:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: model-upload-2
        upload-model-to-gcs-2:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-model-to-gcs-2
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-rf-out_model
            parameters:
              model_name:
                runtimeValue:
                  constant: RF
              model_repo:
                componentInputParameter: pipelinechannel--model_repo
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: upload-model-to-gcs-2
    inputDefinitions:
      artifacts:
        pipelinechannel--train-rf-out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--compare-model-Output:
          parameterType: STRING
        pipelinechannel--model_repo:
          parameterType: STRING
        pipelinechannel--model_repo_uri:
          parameterType: STRING
        pipelinechannel--project_id:
          parameterType: STRING
  comp-condition-3:
    dag:
      tasks:
        importer-3:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-importer-3
          dependentTasks:
          - upload-model-to-gcs-3
          inputs:
            parameters:
              uri:
                componentInputParameter: pipelinechannel--model_repo_uri
          taskInfo:
            name: importer-3
        model-upload-3:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-model-upload-3
          dependentTasks:
          - importer-3
          inputs:
            artifacts:
              unmanaged_container_model:
                taskOutputArtifact:
                  outputArtifactKey: artifact
                  producerTask: importer-3
            parameters:
              display_name:
                runtimeValue:
                  constant: phishing-prediction-model-v3-LR
              project:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: model-upload-3
        upload-model-to-gcs-3:
          cachingOptions:
            enableCache: true
          componentRef:
            name: comp-upload-model-to-gcs-3
          inputs:
            artifacts:
              model:
                componentInputArtifact: pipelinechannel--train-lr-out_model
            parameters:
              model_name:
                runtimeValue:
                  constant: LR
              model_repo:
                componentInputParameter: pipelinechannel--model_repo
              project_id:
                componentInputParameter: pipelinechannel--project_id
          taskInfo:
            name: upload-model-to-gcs-3
    inputDefinitions:
      artifacts:
        pipelinechannel--train-lr-out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        pipelinechannel--compare-model-Output:
          parameterType: STRING
        pipelinechannel--model_repo:
          parameterType: STRING
        pipelinechannel--model_repo_uri:
          parameterType: STRING
        pipelinechannel--project_id:
          parameterType: STRING
  comp-importer:
    executorLabel: exec-importer
    inputDefinitions:
      parameters:
        uri:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        artifact:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
  comp-importer-2:
    executorLabel: exec-importer-2
    inputDefinitions:
      parameters:
        uri:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        artifact:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
  comp-importer-3:
    executorLabel: exec-importer-3
    inputDefinitions:
      parameters:
        uri:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        artifact:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
  comp-load-data:
    executorLabel: exec-load-data
    inputDefinitions:
      parameters:
        bucket:
          parameterType: STRING
        filename:
          parameterType: STRING
        project_id:
          parameterType: STRING
    outputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-model-upload:
    executorLabel: exec-model-upload
    inputDefinitions:
      artifacts:
        parent_model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
          description: An artifact of a model which to upload a new version to. Only
            specify this field when uploading a new version. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload#request-body)
          isOptional: true
        unmanaged_container_model:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
          description: 'The unmanaged container model to be uploaded.  The Model can
            be passed from an upstream step or imported via a KFP `dsl.importer`.
            Example:

            from kfp import dsl

            from google_cloud_pipeline_components.types import artifact_types


            importer_spec = dsl.importer( artifact_uri=''gs://managed-pipeline-gcpc-e2e-test/automl-tabular/model'',
            artifact_class=artifact_types.UnmanagedContainerModel, metadata={ ''containerSpec'':
            { ''imageUri'': ''us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:prod''
            } })'
          isOptional: true
      parameters:
        description:
          defaultValue: ''
          description: The description of the Model. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model)
          isOptional: true
          parameterType: STRING
        display_name:
          description: The display name of the Model. The name can be up to 128 characters
            long and can be consist of any UTF-8 characters. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model)
          parameterType: STRING
        encryption_spec_key_name:
          defaultValue: ''
          description: 'Customer-managed encryption key spec for a Model. If set,
            this Model and all sub-resources of this Model will be secured by this
            key.  Has the form: `projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key`.
            The key needs to be in the same region as where the compute resource is
            created.'
          isOptional: true
          parameterType: STRING
        explanation_metadata:
          defaultValue: {}
          description: Metadata describing the Model's input and output for explanation.
            Both `explanation_metadata` and `explanation_parameters` must be passed
            together when used. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata)
          isOptional: true
          parameterType: STRUCT
        explanation_parameters:
          defaultValue: {}
          description: Parameters to configure explaining for Model's predictions.  [More
            information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationParameters)
          isOptional: true
          parameterType: STRUCT
        labels:
          defaultValue: {}
          description: The labels with user-defined metadata to organize your model.  Label
            keys and values can be no longer than 64 characters (Unicode codepoints),
            can only contain lowercase letters, numeric characters, underscores and
            dashes. International characters are allowed.  See https://goo.gl/xmQnxf
            for more information and examples of labels.
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          description: Optional location to upload this Model to. If not set, defaults
            to `us-central1`.
          isOptional: true
          parameterType: STRING
        project:
          defaultValue: '{{$.pipeline_google_cloud_project_id}}'
          description: Project to upload this Model to. Defaults to the project in
            which the PipelineJob is run.
          isOptional: true
          parameterType: STRING
        version_aliases:
          defaultValue: []
          description: User provided version aliases so that a model version can be
            referenced via alias (i.e. `projects/{project}/locations/{location}/models/{modelId}@{version_alias}`
            instead of auto-generated version id (i.e. `projects/{project}/locations/{location}/models/{modelId}@{versionId}`).
            The format is [a-z][a-zA-Z0-9-]{0,126}[a-z0-9] to distinguish from versionId.
            A default version alias will be created for the first version of the model,
            and there must be exactly one default version alias for a model.
          isOptional: true
          parameterType: LIST
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
          description: Artifact tracking the created Model version.
      parameters:
        gcp_resources:
          description: Serialized JSON of `gcp_resources` [proto](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/proto)
            which tracks the upload Model's long-running operation.
          parameterType: STRING
  comp-model-upload-2:
    executorLabel: exec-model-upload-2
    inputDefinitions:
      artifacts:
        parent_model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
          description: An artifact of a model which to upload a new version to. Only
            specify this field when uploading a new version. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload#request-body)
          isOptional: true
        unmanaged_container_model:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
          description: 'The unmanaged container model to be uploaded.  The Model can
            be passed from an upstream step or imported via a KFP `dsl.importer`.
            Example:

            from kfp import dsl

            from google_cloud_pipeline_components.types import artifact_types


            importer_spec = dsl.importer( artifact_uri=''gs://managed-pipeline-gcpc-e2e-test/automl-tabular/model'',
            artifact_class=artifact_types.UnmanagedContainerModel, metadata={ ''containerSpec'':
            { ''imageUri'': ''us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:prod''
            } })'
          isOptional: true
      parameters:
        description:
          defaultValue: ''
          description: The description of the Model. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model)
          isOptional: true
          parameterType: STRING
        display_name:
          description: The display name of the Model. The name can be up to 128 characters
            long and can be consist of any UTF-8 characters. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model)
          parameterType: STRING
        encryption_spec_key_name:
          defaultValue: ''
          description: 'Customer-managed encryption key spec for a Model. If set,
            this Model and all sub-resources of this Model will be secured by this
            key.  Has the form: `projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key`.
            The key needs to be in the same region as where the compute resource is
            created.'
          isOptional: true
          parameterType: STRING
        explanation_metadata:
          defaultValue: {}
          description: Metadata describing the Model's input and output for explanation.
            Both `explanation_metadata` and `explanation_parameters` must be passed
            together when used. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata)
          isOptional: true
          parameterType: STRUCT
        explanation_parameters:
          defaultValue: {}
          description: Parameters to configure explaining for Model's predictions.  [More
            information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationParameters)
          isOptional: true
          parameterType: STRUCT
        labels:
          defaultValue: {}
          description: The labels with user-defined metadata to organize your model.  Label
            keys and values can be no longer than 64 characters (Unicode codepoints),
            can only contain lowercase letters, numeric characters, underscores and
            dashes. International characters are allowed.  See https://goo.gl/xmQnxf
            for more information and examples of labels.
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          description: Optional location to upload this Model to. If not set, defaults
            to `us-central1`.
          isOptional: true
          parameterType: STRING
        project:
          defaultValue: '{{$.pipeline_google_cloud_project_id}}'
          description: Project to upload this Model to. Defaults to the project in
            which the PipelineJob is run.
          isOptional: true
          parameterType: STRING
        version_aliases:
          defaultValue: []
          description: User provided version aliases so that a model version can be
            referenced via alias (i.e. `projects/{project}/locations/{location}/models/{modelId}@{version_alias}`
            instead of auto-generated version id (i.e. `projects/{project}/locations/{location}/models/{modelId}@{versionId}`).
            The format is [a-z][a-zA-Z0-9-]{0,126}[a-z0-9] to distinguish from versionId.
            A default version alias will be created for the first version of the model,
            and there must be exactly one default version alias for a model.
          isOptional: true
          parameterType: LIST
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
          description: Artifact tracking the created Model version.
      parameters:
        gcp_resources:
          description: Serialized JSON of `gcp_resources` [proto](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/proto)
            which tracks the upload Model's long-running operation.
          parameterType: STRING
  comp-model-upload-3:
    executorLabel: exec-model-upload-3
    inputDefinitions:
      artifacts:
        parent_model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
          description: An artifact of a model which to upload a new version to. Only
            specify this field when uploading a new version. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models/upload#request-body)
          isOptional: true
        unmanaged_container_model:
          artifactType:
            schemaTitle: google.UnmanagedContainerModel
            schemaVersion: 0.0.1
          description: 'The unmanaged container model to be uploaded.  The Model can
            be passed from an upstream step or imported via a KFP `dsl.importer`.
            Example:

            from kfp import dsl

            from google_cloud_pipeline_components.types import artifact_types


            importer_spec = dsl.importer( artifact_uri=''gs://managed-pipeline-gcpc-e2e-test/automl-tabular/model'',
            artifact_class=artifact_types.UnmanagedContainerModel, metadata={ ''containerSpec'':
            { ''imageUri'': ''us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:prod''
            } })'
          isOptional: true
      parameters:
        description:
          defaultValue: ''
          description: The description of the Model. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model)
          isOptional: true
          parameterType: STRING
        display_name:
          description: The display name of the Model. The name can be up to 128 characters
            long and can be consist of any UTF-8 characters. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.models#Model)
          parameterType: STRING
        encryption_spec_key_name:
          defaultValue: ''
          description: 'Customer-managed encryption key spec for a Model. If set,
            this Model and all sub-resources of this Model will be secured by this
            key.  Has the form: `projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key`.
            The key needs to be in the same region as where the compute resource is
            created.'
          isOptional: true
          parameterType: STRING
        explanation_metadata:
          defaultValue: {}
          description: Metadata describing the Model's input and output for explanation.
            Both `explanation_metadata` and `explanation_parameters` must be passed
            together when used. [More information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#explanationmetadata)
          isOptional: true
          parameterType: STRUCT
        explanation_parameters:
          defaultValue: {}
          description: Parameters to configure explaining for Model's predictions.  [More
            information.](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#ExplanationParameters)
          isOptional: true
          parameterType: STRUCT
        labels:
          defaultValue: {}
          description: The labels with user-defined metadata to organize your model.  Label
            keys and values can be no longer than 64 characters (Unicode codepoints),
            can only contain lowercase letters, numeric characters, underscores and
            dashes. International characters are allowed.  See https://goo.gl/xmQnxf
            for more information and examples of labels.
          isOptional: true
          parameterType: STRUCT
        location:
          defaultValue: us-central1
          description: Optional location to upload this Model to. If not set, defaults
            to `us-central1`.
          isOptional: true
          parameterType: STRING
        project:
          defaultValue: '{{$.pipeline_google_cloud_project_id}}'
          description: Project to upload this Model to. Defaults to the project in
            which the PipelineJob is run.
          isOptional: true
          parameterType: STRING
        version_aliases:
          defaultValue: []
          description: User provided version aliases so that a model version can be
            referenced via alias (i.e. `projects/{project}/locations/{location}/models/{modelId}@{version_alias}`
            instead of auto-generated version id (i.e. `projects/{project}/locations/{location}/models/{modelId}@{versionId}`).
            The format is [a-z][a-zA-Z0-9-]{0,126}[a-z0-9] to distinguish from versionId.
            A default version alias will be created for the first version of the model,
            and there must be exactly one default version alias for a model.
          isOptional: true
          parameterType: LIST
    outputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: google.VertexModel
            schemaVersion: 0.0.1
          description: Artifact tracking the created Model version.
      parameters:
        gcp_resources:
          description: Serialized JSON of `gcp_resources` [proto](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/proto)
            which tracks the upload Model's long-running operation.
          parameterType: STRING
  comp-train-lr:
    executorLabel: exec-train-lr
    inputDefinitions:
      artifacts:
        test_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        metrics:
          parameterType: STRUCT
  comp-train-rf:
    executorLabel: exec-train-rf
    inputDefinitions:
      artifacts:
        test_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        metrics:
          parameterType: STRUCT
  comp-train-svm:
    executorLabel: exec-train-svm
    inputDefinitions:
      artifacts:
        test_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        train_set:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        out_model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        metrics:
          parameterType: STRUCT
  comp-train-test-split:
    executorLabel: exec-train-test-split
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
    outputDefinitions:
      artifacts:
        dataset_test:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
        dataset_train:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-upload-model-to-gcs:
    executorLabel: exec-upload-model-to-gcs
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_name:
          parameterType: STRING
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
  comp-upload-model-to-gcs-2:
    executorLabel: exec-upload-model-to-gcs-2
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_name:
          parameterType: STRING
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
  comp-upload-model-to-gcs-3:
    executorLabel: exec-upload-model-to-gcs-3
    inputDefinitions:
      artifacts:
        model:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
      parameters:
        model_name:
          parameterType: STRING
        model_repo:
          parameterType: STRING
        project_id:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-compare-model:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - compare_model
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef compare_model(svm_metrics: dict, rf_metrics: dict, lr_metrics:\
          \ dict) -> str:\n    import logging\n    import json\n    import sys\n \
          \   logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n    logging.info(svm_metrics)\n\
          \    logging.info(rf_metrics)\n    logging.info(lr_metrics)\n\n    svm_recall\
          \ = svm_metrics.get('recall')\n    svm_accuracy = svm_metrics.get('accuracy')\n\
          \n    rf_recall = rf_metrics.get('recall')\n    rf_accuracy = rf_metrics.get('accuracy')\n\
          \n    lr_recall = lr_metrics.get('recall')\n    lr_accuracy = lr_metrics.get('accuracy')\n\
          \n    best_model = None\n    best_recall = 0\n\n    # Compare SVM model\n\
          \    if svm_accuracy >= 0.7 and svm_recall > best_recall:\n        best_model\
          \ = 'SVM'\n        best_recall = svm_recall\n\n    # Compare Random Forest\
          \ model\n    if rf_accuracy >= 0.7 and rf_recall > best_recall:\n      \
          \  best_model = 'RF'\n        best_recall = rf_recall\n\n    # Compare Logistic\
          \ Regression model\n    if lr_accuracy >= 0.7 and lr_recall > best_recall:\n\
          \        best_model = 'LR'\n        best_recall = lr_recall\n\n    if best_model\
          \ is None:\n        if svm_accuracy > lr_accuracy:\n            if svm_accuracy\
          \ > rf_accuracy:\n                best_model = \"SVM\"\n            else:\n\
          \                best_model = \"RF\"\n        else:\n            if lr_accuracy\
          \ > rf_accuracy:\n                best_model = \"LR\"\n            else:\n\
          \                best_model = \"RF\"\n\n    return best_model\n\n"
        image: python:3.10.7-slim
    exec-importer:
      importer:
        artifactUri:
          runtimeParameter: uri
        metadata:
          containerSpec:
            imageUri: us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest
        typeSchema:
          schemaTitle: google.UnmanagedContainerModel
          schemaVersion: 0.0.1
    exec-importer-2:
      importer:
        artifactUri:
          runtimeParameter: uri
        metadata:
          containerSpec:
            imageUri: us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest
        typeSchema:
          schemaTitle: google.UnmanagedContainerModel
          schemaVersion: 0.0.1
    exec-importer-3:
      importer:
        artifactUri:
          runtimeParameter: uri
        metadata:
          containerSpec:
            imageUri: us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest
        typeSchema:
          schemaTitle: google.UnmanagedContainerModel
          schemaVersion: 0.0.1
    exec-load-data:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - load_data
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef load_data(project_id: str, bucket: str, filename: str, dataset:\
          \ Output[Dataset]):\n    '''download data'''\n    from google.cloud import\
          \ storage\n    import pandas as pd\n    import logging \n    import sys\n\
          \n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n   \
          \ # small test\n    assert filename == 'phishing.csv', \"You are trying\
          \ to download an incorrect file. You should download phishing.csv\"\n\n\
          \    # Downloaing the file from a google bucket \n    client = storage.Client(project=project_id)\n\
          \    bucket = client.bucket(bucket)\n    blob = bucket.blob(filename)\n\
          \    blob.download_to_filename(dataset.path + \".csv\")\n    logging.info('Downloaded\
          \ Data!')\n\n"
        image: python:3.10.7-slim
    exec-model-upload:
      container:
        args:
        - --type
        - UploadModel
        - --payload
        - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
          "\"", ", \"description\": \"", "{{$.inputs.parameters[''description'']}}",
          "\"", ", \"explanation_spec\": {", "\"parameters\": ", "{{$.inputs.parameters[''explanation_parameters'']}}",
          ", \"metadata\": ", "{{$.inputs.parameters[''explanation_metadata'']}}",
          "}", ", \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", ", \"version_aliases\": ", "{{$.inputs.parameters[''version_aliases'']}}",
          ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", ", \"pipeline_job\":
          \"", "projects/{{$.inputs.parameters[''project'']}}/locations/{{$.inputs.parameters[''location'']}}/pipelineJobs/{{$.pipeline_job_uuid}}",
          "\"", "}"]}'
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
          "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.model.upload_model.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.17.0
    exec-model-upload-2:
      container:
        args:
        - --type
        - UploadModel
        - --payload
        - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
          "\"", ", \"description\": \"", "{{$.inputs.parameters[''description'']}}",
          "\"", ", \"explanation_spec\": {", "\"parameters\": ", "{{$.inputs.parameters[''explanation_parameters'']}}",
          ", \"metadata\": ", "{{$.inputs.parameters[''explanation_metadata'']}}",
          "}", ", \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", ", \"version_aliases\": ", "{{$.inputs.parameters[''version_aliases'']}}",
          ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", ", \"pipeline_job\":
          \"", "projects/{{$.inputs.parameters[''project'']}}/locations/{{$.inputs.parameters[''location'']}}/pipelineJobs/{{$.pipeline_job_uuid}}",
          "\"", "}"]}'
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
          "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.model.upload_model.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.17.0
    exec-model-upload-3:
      container:
        args:
        - --type
        - UploadModel
        - --payload
        - '{"Concat": ["{", "\"display_name\": \"", "{{$.inputs.parameters[''display_name'']}}",
          "\"", ", \"description\": \"", "{{$.inputs.parameters[''description'']}}",
          "\"", ", \"explanation_spec\": {", "\"parameters\": ", "{{$.inputs.parameters[''explanation_parameters'']}}",
          ", \"metadata\": ", "{{$.inputs.parameters[''explanation_metadata'']}}",
          "}", ", \"encryption_spec\": {\"kms_key_name\":\"", "{{$.inputs.parameters[''encryption_spec_key_name'']}}",
          "\"}", ", \"version_aliases\": ", "{{$.inputs.parameters[''version_aliases'']}}",
          ", \"labels\": ", "{{$.inputs.parameters[''labels'']}}", ", \"pipeline_job\":
          \"", "projects/{{$.inputs.parameters[''project'']}}/locations/{{$.inputs.parameters[''location'']}}/pipelineJobs/{{$.pipeline_job_uuid}}",
          "\"", "}"]}'
        - --project
        - '{{$.inputs.parameters[''project'']}}'
        - --location
        - '{{$.inputs.parameters[''location'']}}'
        - --gcp_resources
        - '{{$.outputs.parameters[''gcp_resources''].output_file}}'
        - --executor_input
        - '{{$}}'
        - '{"IfPresent": {"InputName": "parent_model", "Then": ["--parent_model_name",
          "{{$.inputs.artifacts[''parent_model''].metadata[''resourceName'']}}"]}}'
        command:
        - python3
        - -u
        - -m
        - google_cloud_pipeline_components.container.v1.model.upload_model.launcher
        image: gcr.io/ml-pipeline/google-cloud-pipeline-components:2.17.0
    exec-train-lr:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_lr
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.3.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_lr(train_set: Input[Dataset], test_set: Input[Dataset],\
          \ out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n  \
          \  '''train a Logistic Regression with default parameters'''\n    import\
          \ pandas as pd\n    from sklearn.linear_model import LogisticRegression\n\
          \    from sklearn.metrics import recall_score, accuracy_score\n    import\
          \ json\n    import logging \n    import sys\n    import os\n    import pickle\
          \  \n\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n\
          \    df_train = pd.read_csv(train_set.path+\".csv\")\n    df_test = pd.read_csv(test_set.path+\"\
          .csv\")\n\n    logging.info(df_train.columns)\n    logging.info(df_test.columns)\
          \  \n\n    x_train, y_train = df_train.drop('CLASS_LABEL', axis=1), df_train['CLASS_LABEL']\n\
          \    x_test, y_test = df_test.drop('CLASS_LABEL', axis=1), df_test['CLASS_LABEL']\n\
          \n    # df = pd.read_csv(features.path)\n\n    # logging.info(df.columns)\
          \        \n\n    # x_train, x_test, y_train, y_test = train_test_split(df.drop('CLASS_LABEL',axis=1),\
          \ \n    #                                                 df['CLASS_LABEL'],\
          \ test_size=0.20, \n    #                                              \
          \   random_state=42)\n\n    model_lr = LogisticRegression(random_state=42)\n\
          \    model_lr.fit(x_train,y_train)\n    y_pred = model_lr.predict(x_test)\n\
          \n    metrics_dict = {\n        \"accuracy\": accuracy_score(y_test, y_pred),\n\
          \        \"recall\": recall_score(y_test, y_pred)\n    }\n    logging.info(metrics_dict)\
          \  \n\n    out_model.metadata[\"file_type\"] = \".pkl\"\n    out_model.metadata[\"\
          algorithm\"] = \"LR\"\n   # Save the model\n    model_file = out_model.path\
          \ + \".pkl\"\n    with open(model_file, 'wb') as f:  \n        pickle.dump(model_lr,\
          \ f)   \n\n    outputs = NamedTuple('outputs', metrics=dict)\n    return\
          \ outputs(metrics_dict)\n\n"
        image: python:3.10.7-slim
    exec-train-rf:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_rf
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.3.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_rf(train_set: Input[Dataset], test_set: Input[Dataset],\
          \ out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n  \
          \  '''train a Random Forest with default parameters'''\n    import pandas\
          \ as pd\n    from sklearn.ensemble import RandomForestClassifier\n    from\
          \ sklearn.metrics import recall_score, accuracy_score\n    import json\n\
          \    import logging \n    import sys\n    import os\n    import pickle \
          \ \n\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n\
          \    df_train = pd.read_csv(train_set.path+\".csv\")\n    df_test = pd.read_csv(test_set.path+\"\
          .csv\")\n\n    logging.info(df_train.columns)\n    logging.info(df_test.columns)\
          \  \n\n    x_train, y_train = df_train.drop('CLASS_LABEL', axis=1), df_train['CLASS_LABEL']\n\
          \    x_test, y_test = df_test.drop('CLASS_LABEL', axis=1), df_test['CLASS_LABEL']\n\
          \n    # df = pd.read_csv(features.path)\n\n    # logging.info(df.columns)\
          \        \n\n    # x_train, x_test, y_train, y_test = train_test_split(df.drop('CLASS_LABEL',axis=1),\
          \ \n    #                                                 df['CLASS_LABEL'],\
          \ test_size=0.20, \n    #                                              \
          \   random_state=42)\n\n    model_rf = RandomForestClassifier(random_state=42)\
          \ \n    model_rf.fit(x_train,y_train)\n    y_pred = model_rf.predict(x_test)\n\
          \n    metrics_dict = {\n        \"accuracy\": accuracy_score(y_test, y_pred),\n\
          \        \"recall\": recall_score(y_test, y_pred)\n    }\n    logging.info(metrics_dict)\
          \  \n\n    out_model.metadata[\"file_type\"] = \".pkl\"\n    out_model.metadata[\"\
          algorithm\"] = \"RF\"\n   # Save the model\n    model_file = out_model.path\
          \ + \".pkl\"\n    with open(model_file, 'wb') as f:  \n        pickle.dump(model_rf,\
          \ f)   \n\n    outputs = NamedTuple('outputs', metrics=dict)\n    return\
          \ outputs(metrics_dict)\n\n"
        image: python:3.10.7-slim
    exec-train-svm:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_svm
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.3.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_svm(train_set: Input[Dataset], test_set: Input[Dataset],\
          \ out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n  \
          \  '''train a SVM with default parameters'''\n    import pandas as pd\n\
          \    from sklearn import svm\n    from sklearn import metrics\n    from\
          \ sklearn.metrics import recall_score, accuracy_score\n    import json\n\
          \    import logging \n    import sys\n    import os\n    import pickle \
          \ \n\n    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n\n\
          \    df_train = pd.read_csv(train_set.path+\".csv\")\n    df_test = pd.read_csv(test_set.path+\"\
          .csv\")\n\n    logging.info(df_train.columns)\n    logging.info(df_test.columns)\
          \  \n\n    x_train, y_train = df_train.drop('CLASS_LABEL', axis=1), df_train['CLASS_LABEL']\n\
          \    x_test, y_test = df_test.drop('CLASS_LABEL', axis=1), df_test['CLASS_LABEL']\n\
          \n    # x_train, x_test, y_train, y_test = train_test_split(df.drop('CLASS_LABEL',axis=1),\
          \ \n    #                                                 df['CLASS_LABEL'],\
          \ test_size=0.20, \n    #                                              \
          \   random_state=42)\n    model_svm = svm.SVC(random_state=42)\n    model_svm.fit(x_train,y_train)\n\
          \    y_pred = model_svm.predict(x_test)\n\n    metrics_dict = {\n      \
          \  \"accuracy\": accuracy_score(y_test, y_pred),\n        \"recall\": recall_score(y_test,\
          \ y_pred)\n    }\n    logging.info(metrics_dict)  \n\n    out_model.metadata[\"\
          file_type\"] = \".pkl\"\n    out_model.metadata[\"algorithm\"] = \"SVM\"\
          \n   # Save the model\n    model_file = out_model.path + \".pkl\"\n    with\
          \ open(model_file, 'wb') as f:  \n        pickle.dump(model_svm, f)   \n\
          \n    outputs = NamedTuple('outputs', metrics=dict)\n    return outputs(metrics_dict)\n\
          \n"
        image: python:3.10.7-slim
    exec-train-test-split:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_test_split
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'pandas' 'scikit-learn==1.3.2'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_test_split(dataset: Input[Dataset], dataset_train: Output[Dataset],\
          \ dataset_test: Output[Dataset]):\n    '''train_test_split'''\n    import\
          \ pandas as pd\n    import logging \n    import sys\n    from sklearn.model_selection\
          \ import train_test_split as tts\n\n    logging.basicConfig(stream=sys.stdout,\
          \ level=logging.INFO) \n\n    alldata = pd.read_csv(dataset.path+\".csv\"\
          , index_col=None)\n    train, test = tts(alldata, test_size=0.2, random_state=42)\n\
          \    train.to_csv(dataset_train.path + \".csv\" , index=False, encoding='utf-8-sig')\n\
          \    test.to_csv(dataset_test.path + \".csv\" , index=False, encoding='utf-8-sig')\n\
          \n"
        image: python:3.10.7-slim
    exec-upload-model-to-gcs:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_gcs(project_id: str, model_repo: str, model:\
          \ Input[Model], model_name: str):\n    '''upload model to gsc'''\n    from\
          \ google.cloud import storage   \n    import logging \n    import sys\n\n\
          \    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n\n\
          \    # upload the model to GCS\n    client = storage.Client(project=project_id)\n\
          \    bucket = client.bucket(model_repo)\n    blob = bucket.blob('model-A1-v3.pkl')\n\
          \    source_file_name= model.path + '.pkl'\n\n    blob.upload_from_filename(source_file_name)\
          \    \n\n    print(f\"File {source_file_name} uploaded to {model_repo}.\"\
          )\n\n"
        image: python:3.10.7-slim
    exec-upload-model-to-gcs-2:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_gcs(project_id: str, model_repo: str, model:\
          \ Input[Model], model_name: str):\n    '''upload model to gsc'''\n    from\
          \ google.cloud import storage   \n    import logging \n    import sys\n\n\
          \    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n\n\
          \    # upload the model to GCS\n    client = storage.Client(project=project_id)\n\
          \    bucket = client.bucket(model_repo)\n    blob = bucket.blob('model-A1-v3.pkl')\n\
          \    source_file_name= model.path + '.pkl'\n\n    blob.upload_from_filename(source_file_name)\
          \    \n\n    print(f\"File {source_file_name} uploaded to {model_repo}.\"\
          )\n\n"
        image: python:3.10.7-slim
    exec-upload-model-to-gcs-3:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - upload_model_to_gcs
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.7.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-storage'\
          \ && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef upload_model_to_gcs(project_id: str, model_repo: str, model:\
          \ Input[Model], model_name: str):\n    '''upload model to gsc'''\n    from\
          \ google.cloud import storage   \n    import logging \n    import sys\n\n\
          \    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n\n\
          \    # upload the model to GCS\n    client = storage.Client(project=project_id)\n\
          \    bucket = client.bucket(model_repo)\n    blob = bucket.blob('model-A1-v3.pkl')\n\
          \    source_file_name= model.path + '.pkl'\n\n    blob.upload_from_filename(source_file_name)\
          \    \n\n    print(f\"File {source_file_name} uploaded to {model_repo}.\"\
          )\n\n"
        image: python:3.10.7-slim
pipelineInfo:
  name: phishing-predictor-training-pipeline-v3
root:
  dag:
    tasks:
      compare-model:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-compare-model
        dependentTasks:
        - train-lr
        - train-rf
        - train-svm
        inputs:
          parameters:
            lr_metrics:
              taskOutputParameter:
                outputParameterKey: metrics
                producerTask: train-lr
            rf_metrics:
              taskOutputParameter:
                outputParameterKey: metrics
                producerTask: train-rf
            svm_metrics:
              taskOutputParameter:
                outputParameterKey: metrics
                producerTask: train-svm
        taskInfo:
          name: compare-model
      condition-1:
        componentRef:
          name: comp-condition-1
        dependentTasks:
        - compare-model
        - train-svm
        inputs:
          artifacts:
            pipelinechannel--train-svm-out_model:
              taskOutputArtifact:
                outputArtifactKey: out_model
                producerTask: train-svm
          parameters:
            pipelinechannel--compare-model-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: compare-model
            pipelinechannel--model_repo:
              componentInputParameter: model_repo
            pipelinechannel--model_repo_uri:
              componentInputParameter: model_repo_uri
            pipelinechannel--project_id:
              componentInputParameter: project_id
        taskInfo:
          name: condition-1
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--compare-model-Output']
            == 'SVM'
      condition-2:
        componentRef:
          name: comp-condition-2
        dependentTasks:
        - compare-model
        - train-rf
        inputs:
          artifacts:
            pipelinechannel--train-rf-out_model:
              taskOutputArtifact:
                outputArtifactKey: out_model
                producerTask: train-rf
          parameters:
            pipelinechannel--compare-model-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: compare-model
            pipelinechannel--model_repo:
              componentInputParameter: model_repo
            pipelinechannel--model_repo_uri:
              componentInputParameter: model_repo_uri
            pipelinechannel--project_id:
              componentInputParameter: project_id
        taskInfo:
          name: condition-2
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--compare-model-Output']
            == 'RF'
      condition-3:
        componentRef:
          name: comp-condition-3
        dependentTasks:
        - compare-model
        - train-lr
        inputs:
          artifacts:
            pipelinechannel--train-lr-out_model:
              taskOutputArtifact:
                outputArtifactKey: out_model
                producerTask: train-lr
          parameters:
            pipelinechannel--compare-model-Output:
              taskOutputParameter:
                outputParameterKey: Output
                producerTask: compare-model
            pipelinechannel--model_repo:
              componentInputParameter: model_repo
            pipelinechannel--model_repo_uri:
              componentInputParameter: model_repo_uri
            pipelinechannel--project_id:
              componentInputParameter: project_id
        taskInfo:
          name: condition-3
        triggerPolicy:
          condition: inputs.parameter_values['pipelinechannel--compare-model-Output']
            == 'LR'
      load-data:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-load-data
        inputs:
          parameters:
            bucket:
              componentInputParameter: data_bucket
            filename:
              componentInputParameter: filename
            project_id:
              componentInputParameter: project_id
        taskInfo:
          name: load-data
      train-lr:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-lr
        dependentTasks:
        - train-test-split
        inputs:
          artifacts:
            test_set:
              taskOutputArtifact:
                outputArtifactKey: dataset_test
                producerTask: train-test-split
            train_set:
              taskOutputArtifact:
                outputArtifactKey: dataset_train
                producerTask: train-test-split
        taskInfo:
          name: train-lr
      train-rf:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-rf
        dependentTasks:
        - train-test-split
        inputs:
          artifacts:
            test_set:
              taskOutputArtifact:
                outputArtifactKey: dataset_test
                producerTask: train-test-split
            train_set:
              taskOutputArtifact:
                outputArtifactKey: dataset_train
                producerTask: train-test-split
        taskInfo:
          name: train-rf
      train-svm:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-svm
        dependentTasks:
        - train-test-split
        inputs:
          artifacts:
            test_set:
              taskOutputArtifact:
                outputArtifactKey: dataset_test
                producerTask: train-test-split
            train_set:
              taskOutputArtifact:
                outputArtifactKey: dataset_train
                producerTask: train-test-split
        taskInfo:
          name: train-svm
      train-test-split:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-test-split
        dependentTasks:
        - load-data
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: dataset
                producerTask: load-data
        taskInfo:
          name: train-test-split
  inputDefinitions:
    parameters:
      data_bucket:
        parameterType: STRING
      filename:
        parameterType: STRING
      model_repo:
        parameterType: STRING
      model_repo_uri:
        parameterType: STRING
      project_id:
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.7.0
