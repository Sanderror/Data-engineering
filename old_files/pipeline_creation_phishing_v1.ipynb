{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52627d51",
   "metadata": {},
   "source": [
    "### Make sure to first upload the Iris.csv file to your data bucket in cloud storage to be able to run this file\n",
    "\n",
    "And if you want to do the batch prediction, also add the iris_batch.csv file to that data bucket. Or another batch file you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7094683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp>2\n",
      "  Downloading kfp-2.9.0.tar.gz (595 kB)\n",
      "     ------------------------------------- 595.6/595.6 kB 20.8 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting google-cloud-pipeline-components>2\n",
      "  Downloading google_cloud_pipeline_components-2.17.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting google-cloud-aiplatform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oblib (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -olorama (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -sync-timeout (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Error parsing dependencies of pyodbc: Invalid version: '4.0.0-unsupported'\n",
      "WARNING: Error parsing dependencies of pyzmq: Invalid version: 'cpython'\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -oblib (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -olorama (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -sync-timeout (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -mpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "  WARNING: The script wsdump.exe is installed in 'C:\\Users\\20213574\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tabulate.exe is installed in 'C:\\Users\\20213574\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts dsl-compile-deprecated.exe, dsl-compile.exe and kfp.exe are installed in 'C:\\Users\\20213574\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tb-gcp-uploader.exe is installed in 'C:\\Users\\20213574\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.\n",
      "jupyter-server 1.4.1 requires pyzmq>=17, which is not installed.\n",
      "notebook 6.3.0 requires pyzmq>=17, which is not installed.\n",
      "spyder 4.2.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 4.2.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "spyder 4.2.5 requires pyzmq>=17, which is not installed.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.7.1 which is incompatible.\n",
      "sphinx 4.0.1 requires Jinja2<3.0,>=2.3, but you have jinja2 3.1.4 which is incompatible.\n",
      "sphinx 4.0.1 requires MarkupSafe<2.0, but you have markupsafe 2.1.5 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading google_cloud_aiplatform-1.70.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (8.1.3)\n",
      "Collecting docstring-parser<1,>=0.7.3 (from kfp>2)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from kfp>2)\n",
      "  Downloading google_api_core-2.21.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (2.22.0)\n",
      "Collecting google-cloud-storage<3,>=2.2.1 (from kfp>2)\n",
      "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting kfp-pipeline-spec==0.4.0 (from kfp>2)\n",
      "  Downloading kfp_pipeline_spec-0.4.0-py3-none-any.whl.metadata (301 bytes)\n",
      "Collecting kfp-server-api<2.4.0,>=2.1.0 (from kfp>2)\n",
      "  Downloading kfp_server_api-2.3.0.tar.gz (84 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting kubernetes<31,>=8.0.0 (from kfp>2)\n",
      "  Downloading kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (4.24.2)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (6.0.1)\n",
      "Collecting requests-toolbelt<1,>=0.8.0 (from kfp>2)\n",
      "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tabulate<1,>=0.8.6 (from kfp>2)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from kfp>2) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (4.7.1)\n",
      "Collecting kfp>2\n",
      "  Downloading kfp-2.7.0.tar.gz (441 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting Jinja2<4,>=3.1.2 (from google-cloud-pipeline-components>2)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting kfp-pipeline-spec==0.3.0 (from kfp>2)\n",
      "  Downloading kfp_pipeline_spec-0.3.0-py3-none-any.whl.metadata (329 bytes)\n",
      "Collecting kfp-server-api<2.1.0,>=2.0.0 (from kfp>2)\n",
      "  Downloading kfp-server-api-2.0.5.tar.gz (63 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting kubernetes<27,>=8.0.0 (from kfp>2)\n",
      "  Downloading kubernetes-26.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-aiplatform)\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Collecting google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3 (from google-cloud-aiplatform)\n",
      "  Downloading google_cloud_resource_manager-1.12.5-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-aiplatform) (2.0.1)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-aiplatform) (2.5.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from click<9,>=8.0.0->kfp>2) (0.4.6)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (2.25.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.57.0)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp>2) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp>2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp>2) (4.7.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp>2) (1.15.0)\n",
      "Collecting google-cloud-core<3.0.0dev,>=2.4.1 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media<3.0dev,>=2.0.0 (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.1)\n",
      "Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4 (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform)\n",
      "  Downloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-auth<3,>=1.6.1 (from kfp>2)\n",
      "  Downloading google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage<3,>=2.2.1->kfp>2)\n",
      "  Downloading google_crc32c-1.5.0-cp38-cp38-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components>2)\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp>2) (2020.12.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from kubernetes<27,>=8.0.0->kfp>2) (69.0.2)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes<27,>=8.0.0->kfp>2)\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\programdata\\anaconda3\\lib\\site-packages (from kubernetes<27,>=8.0.0->kfp>2) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from pydantic<3->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3->google-cloud-aiplatform) (2.14.5)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\20213574\\appdata\\roaming\\python\\python38\\site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.22.4)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform)\n",
      "  Downloading grpcio-1.67.0-cp38-cp38-win_amd64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp>2) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (2.10)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp>2) (3.2.2)\n",
      "Downloading google_cloud_pipeline_components-2.17.0-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 1.5/1.5 MB 15.4 MB/s eta 0:00:00\n",
      "Downloading kfp_pipeline_spec-0.3.0-py3-none-any.whl (12 kB)\n",
      "Downloading google_cloud_aiplatform-1.70.0-py2.py3-none-any.whl (5.3 MB)\n",
      "   ---------------------------------------- 5.3/5.3 MB 16.0 MB/s eta 0:00:00\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading google_api_core-2.21.0-py3-none-any.whl (156 kB)\n",
      "Downloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl (239 kB)\n",
      "Downloading google_cloud_resource_manager-1.12.5-py2.py3-none-any.whl (341 kB)\n",
      "Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
      "Downloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading kubernetes-26.1.0-py2.py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 1.4/1.4 MB 19.1 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.5.0-cp38-cp38-win_amd64.whl (27 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading grpc_google_iam_v1-0.13.1-py2.py3-none-any.whl (24 kB)\n",
      "Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Downloading grpcio-1.67.0-cp38-cp38-win_amd64.whl (4.4 MB)\n",
      "   ---------------------------------------- 4.4/4.4 MB 14.6 MB/s eta 0:00:00\n",
      "Downloading MarkupSafe-2.1.5-cp38-cp38-win_amd64.whl (17 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Building wheels for collected packages: kfp, kfp-server-api\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-2.7.0-py3-none-any.whl size=610438 sha256=6420486fc8419a40cb9393c6b1b09ed04ebd6482f19716ae78c9bdcf47c3a4b7\n",
      "  Stored in directory: C:\\Users\\20213574\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-cmmzsl3a\\wheels\\7f\\19\\1f\\08f0ad28f578bbd33557a6e0fb36e2dfb5eb59b9affcf0277e\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-2.0.5-py3-none-any.whl size=114745 sha256=6d253a7009842c7c0ad82af7892d9402abfdc2b59862b33a97b783f445742549\n",
      "  Stored in directory: C:\\Users\\20213574\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-cmmzsl3a\\wheels\\c3\\9d\\21\\1c142a79336693ed381d9e4150351d111fadc6cd519e998b61\n",
      "Successfully built kfp kfp-server-api\n",
      "Installing collected packages: websocket-client, tabulate, proto-plus, MarkupSafe, kfp-pipeline-spec, grpcio, googleapis-common-protos, google-crc32c, docstring-parser, requests-toolbelt, kfp-server-api, Jinja2, grpcio-status, google-resumable-media, google-auth, kubernetes, grpc-google-iam-v1, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, kfp, google-cloud-aiplatform, google-cloud-pipeline-components\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.57.0\n",
      "    Uninstalling grpcio-1.57.0:\n",
      "      Successfully uninstalled grpcio-1.57.0\n",
      "  Attempting uninstall: google-auth\n",
      "    Found existing installation: google-auth 2.22.0\n",
      "    Uninstalling google-auth-2.22.0:\n",
      "      Successfully uninstalled google-auth-2.22.0\n",
      "Successfully installed Jinja2-3.1.4 MarkupSafe-2.1.5 docstring-parser-0.16 google-api-core-2.21.0 google-auth-2.35.0 google-cloud-aiplatform-1.70.0 google-cloud-bigquery-3.26.0 google-cloud-core-2.4.1 google-cloud-pipeline-components-2.17.0 google-cloud-resource-manager-1.12.5 google-cloud-storage-2.18.2 google-crc32c-1.5.0 google-resumable-media-2.7.2 googleapis-common-protos-1.65.0 grpc-google-iam-v1-0.13.1 grpcio-1.67.0 grpcio-status-1.62.3 kfp-2.7.0 kfp-pipeline-spec-0.3.0 kfp-server-api-2.0.5 kubernetes-26.1.0 proto-plus-1.24.0 requests-toolbelt-0.10.1 tabulate-0.9.0 websocket-client-1.8.0\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7784286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581f8117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n",
      "'grep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7b608be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-10d0cfd53301>:17: FutureWarning:  Google Cloud Pipeline Components will drop support for Python 3.8 on Oct 1, 2024. To use new versions of the GCPC SDK after that date, you will need to upgrade to Python >= 3.9. See https://devguide.python.org/versions/ for more details.\n",
      "  from google_cloud_pipeline_components.v1.model import ModelUploadOp\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,ModelDeployOp)\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "331825b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"your project id\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"your url to pipeline root\"   # e.g., gs://temp_de2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af121d",
   "metadata": {},
   "source": [
    "## First create a function that trains a SVM model on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc9fec7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 123] The filename, directory name, or volume label syntax is incorrect: '<ipython-input-6-64505418d412>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-64505418d412>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbase_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"python:3.10.7-slim\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_svm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_model\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mNamedTuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'outputs'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;34m'''train a SVM with default parameters'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\kfp\\dsl\\component_decorator.py\u001b[0m in \u001b[0;36mcomponent\u001b[1;34m(func, base_image, target_image, packages_to_install, pip_index_urls, output_component_file, install_kfp_package, kfp_package_path)\u001b[0m\n\u001b[0;32m    117\u001b[0m             kfp_package_path=kfp_package_path)\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     return component_factory.create_component_from_func(\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mbase_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_image\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\kfp\\dsl\\component_factory.py\u001b[0m in \u001b[0;36mcreate_component_from_func\u001b[1;34m(func, base_image, target_image, packages_to_install, pip_index_urls, output_component_file, install_kfp_package, kfp_package_path)\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[0mmodule_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsourcefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m     \u001b[0mmodule_path\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[0mcomponent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_python_function_name_to_component_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36mresolve\u001b[1;34m(self, strict)\u001b[0m\n\u001b[0;32m   1178\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flavour\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;31m# No symlink resolution => for consistency, raise an error if\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\pathlib.py\u001b[0m in \u001b[0;36mresolve\u001b[1;34m(self, path, strict)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ext_to_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_getfinalpathname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m                         \u001b[0mprevious_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 123] The filename, directory name, or volume label syntax is incorrect: '<ipython-input-6-64505418d412>'"
     ]
    }
   ],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_svm(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a SVM with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn import svm\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import recall_score, accuracy_score\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.read_csv(features.path)\n",
    "    df = df.drop(columns=['id']) # have to drop the variable\n",
    "    \n",
    "    logging.info(df.columns)        \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.drop('CLASS_LABEL',axis=1), \n",
    "                                                    df['CLASS_LABEL'], test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "    model_svm = svm.SVC()\n",
    "    model_svm.fit(x_train,y_train)\n",
    "    y_pred = model_svm.predict(x_test)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred)\n",
    "        \"recall\": recall_score(y_test, y_pred)\n",
    "    }\n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"svm\"\n",
    "   # Save the model\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(model_svm, f)   \n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e104459",
   "metadata": {},
   "source": [
    "## Then train a Random Forest Classifier on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dcb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_rf(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a Random Forest with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import recall_score, accuracy_score\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.read_csv(features.path)\n",
    "    df = df.drop(columns=['id'])\n",
    "    \n",
    "    logging.info(df.columns)        \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.drop('CLASS_LABEL',axis=1), \n",
    "                                                    df['CLASS_LABEL'], test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "    \n",
    "    model_rf = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "    model_rf.fit(x_train,y_train)\n",
    "    y_pred = model_rf.predict(x_test)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred)\n",
    "        \"recall\": recall_score(y_test, y_pred)\n",
    "    }\n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"dt\"\n",
    "   # Save the model\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(model_rf, f)   \n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ccc549",
   "metadata": {},
   "source": [
    "## Then train a Logistic Regression on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a80c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_lr(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a Logistic Regression with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import recall_score, accuracy_score\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.read_csv(features.path)\n",
    "    df = df.drop(columns=['id'])\n",
    "    \n",
    "    logging.info(df.columns)        \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.drop('CLASS_LABEL',axis=1), \n",
    "                                                    df['CLASS_LABEL'], test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "    \n",
    "    model_lr = LogisticRegression(random_state=42)\n",
    "    model_lr.fit(x_train,y_train)\n",
    "    y_pred = model_lr.predict(x_test)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred)\n",
    "        \"recall\": recall_score(y_test, y_pred)\n",
    "    }\n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"dt\"\n",
    "   # Save the model\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(model_lr, f)   \n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdad3f",
   "metadata": {},
   "source": [
    "## Then we compare the two model's performances to decide which one to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35da0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def compare_model(svm_metrics: dict, rf_metrics: dict, lr_metrics: dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(svm_metrics)\n",
    "    logging.info(rf_metrics)\n",
    "    logging.info(lr_metrics)\n",
    "    if svm_metrics.get(\"accuracy\") > dt_metrics.get(\"accuracy\"):\n",
    "        return \"SVM\"\n",
    "    else :\n",
    "        return \"DT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2dc14",
   "metadata": {},
   "source": [
    "## Then we upload the model to google cloud storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394054fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model], model_name: str):\n",
    "    '''upload model to gsc'''\n",
    "    from google.cloud import storage   \n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(model_repo)\n",
    "    blob = bucket.blob('model.pkl')\n",
    "    source_file_name= model.path + '.pkl'\n",
    "   \n",
    "    blob.upload_from_filename(source_file_name)    \n",
    "    \n",
    "    print(f\"File {source_file_name} uploaded to {model_repo}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d3ba3",
   "metadata": {},
   "source": [
    "## Then we create the complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f2531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"phishing-predictor-training-pipeline-v2\")\n",
    "def pipeline(project_id: str, data_bucket: str, dataset_uri: str, model_repo: str, model_repo_uri: str):\n",
    "    \n",
    "    # load the dataset\n",
    "    dataset_op = kfp.dsl.importer(\n",
    "        artifact_uri=dataset_uri,\n",
    "        artifact_class=Dataset,\n",
    "        reimport=False,\n",
    "    )\n",
    "    \n",
    "    # svm model\n",
    "    training_svm_job_run_op = train_svm(\n",
    "        features=dataset_op.output\n",
    "    )\n",
    "    \n",
    "    # rf model\n",
    "    training_rf_job_run_op = train_rf(\n",
    "        features=dataset_op.output\n",
    "    )\n",
    "    \n",
    "    # lr model\n",
    "    training_lr_job_run_op = train_lr(\n",
    "        features=dataset_op.output\n",
    "    )\n",
    "        \n",
    "    # compare models\n",
    "    comp_model_op = compare_model(svm_metrics=training_svm_job_run_op.outputs[\"metrics\"],\n",
    "                                    rf_metrics=training_rf_job_run_op.outputs[\"metrics\"],\n",
    "                                    lr_metrics=training_lr_job_run_op.outputs[\"metrics\"]).after(training_svm_job_run_op, training_rf_job_run_op, training_lr_job_run_op)  \n",
    "    \n",
    "    # This part of the code did not work, so right now, we just use the DT model, whether it was better or not\n",
    "#     with dsl.If(comp_model_op.output=='SVM'):\n",
    "#         final_model_op = training_svm_job_run_op\n",
    "#     with dsl.If(comp_model_op.output=='DT'):\n",
    "#         final_model_op = training_dt_job_run_op\n",
    "    \n",
    "    # upload model to storage\n",
    "    upload_model_to_gc_op = upload_model_to_gcs(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,\n",
    "        model=training_rf_job_run_op.outputs['out_model'],\n",
    "        model_name=comp_model_op.output\n",
    "    ) \n",
    "    \n",
    "    # dont know why we do this but is for deploying the model i believe\n",
    "    import_unmanaged_model_task = dsl.importer(\n",
    "            artifact_uri=model_repo_uri,\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\",  # see https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers  \n",
    "                },\n",
    "            },\n",
    "        ).after(upload_model_to_gc_op)  \n",
    "    \n",
    "    # upload model to model registry\n",
    "    model_upload_op = ModelUploadOp(\n",
    "            project=project_id,\n",
    "            display_name=f\"phishing-prediction-model-{comp_model_op.output}\",\n",
    "            unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "        ).after(import_unmanaged_model_task) \n",
    "    \n",
    "    # create endpoint for predictions\n",
    "    create_endpoint_op = EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            display_name=\"phishing-prediction-service\",\n",
    "        ).after(model_upload_op) \n",
    "    \n",
    "    # deploy full model\n",
    "    model_deploy_op = ModelDeployOp(\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "            deployed_model_display_name=f\"phishing-prediction-model-{comp_model_op.output}\",\n",
    "            dedicated_resources_machine_type=\"n1-standard-2\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            traffic_split={\"0\": 100},\n",
    "        ).after(create_endpoint_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb72f8",
   "metadata": {},
   "source": [
    "## Then we compile the YAML file, which is used to run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='iris_predictor_training_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d4b95",
   "metadata": {},
   "source": [
    "## Then we run the pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cc7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"phishing-predictor\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"phishing_predictor_training_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID, # makesure to use your project id \n",
    "        'data_bucket': 'data_de2024_2083033',  # makesure to use your data bucket name \n",
    "        'dataset_uri':'gs://data_de2024_2083033/phishing.csv',\n",
    "        'model_repo':'models_de2024_2083033', # makesure to use your model bucket name \n",
    "        'model_repo_uri':'gs://models_de2024_2083033' # makesure to use your model bucket name \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e443112c",
   "metadata": {},
   "source": [
    "## Small test to see if model is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f5392",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"phishing-prediction-model\"\n",
    "! gcloud ai models list --region={REGION} --filter={DISPLAY_NAME}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
