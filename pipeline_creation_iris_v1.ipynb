{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52627d51",
   "metadata": {},
   "source": [
    "### Make sure to first upload the Iris.csv file to your data bucket in cloud storage to be able to run this file\n",
    "\n",
    "And if you want to do the batch prediction, also add the iris_batch.csv file to that data bucket. Or another batch file you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7094683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp>2 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (2.7.0)\n",
      "Collecting kfp>2\n",
      "  Downloading kfp-2.9.0.tar.gz (595 kB)\n",
      "     ---------------------------------------- 0.0/595.6 kB ? eta -:--:--\n",
      "     --------------- ---------------------- 245.8/595.6 kB 7.4 MB/s eta 0:00:01\n",
      "     ----------------------------- -------- 460.8/595.6 kB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 595.6/595.6 kB 4.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: google-cloud-pipeline-components>2 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (2.17.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (1.70.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (2.21.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (2.35.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (2.18.2)\n",
      "Collecting kfp-pipeline-spec==0.4.0 (from kfp>2)\n",
      "  Obtaining dependency information for kfp-pipeline-spec==0.4.0 from https://files.pythonhosted.org/packages/0b/d7/422c6e40880a15745eaebf74b5ea9a9bfd8e4d3a62cbb34022917e216468/kfp_pipeline_spec-0.4.0-py3-none-any.whl.metadata\n",
      "  Downloading kfp_pipeline_spec-0.4.0-py3-none-any.whl.metadata (301 bytes)\n",
      "Collecting kfp-server-api<2.4.0,>=2.1.0 (from kfp>2)\n",
      "  Downloading kfp_server_api-2.3.0.tar.gz (84 kB)\n",
      "     ---------------------------------------- 0.0/84.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 84.0/84.0 kB ? eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: kubernetes<31,>=8.0.0 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (26.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (4.25.5)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from kfp>2) (0.8.10)\n",
      "Requirement already satisfied: urllib3<2.0.0 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from kfp>2) (1.26.16)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from kfp>2) (4.10.0)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-pipeline-components>2) (3.1.2)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp>2) (2.0.5)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-aiplatform) (1.24.0)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-aiplatform) (23.1)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-aiplatform) (3.26.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-aiplatform) (1.12.5)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\users\\sande\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (2.0.0)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (2.6.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from click<9,>=8.0.0->kfp>2) (0.4.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (2.31.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.67.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (1.62.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp>2) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp>2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp>2) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from google-cloud-storage<3,>=2.2.1->kfp>2) (1.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components>2) (2.1.3)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp>2) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp>2) (2023.7.22)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from kubernetes<31,>=8.0.0->kfp>2) (68.0.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from kubernetes<31,>=8.0.0->kfp>2) (0.58.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\sande\\anaconda3\\lib\\site-packages (from kubernetes<31,>=8.0.0->kfp>2) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from pydantic<3->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from pydantic<3->google-cloud-aiplatform) (2.16.3)\n",
      "Requirement already satisfied: numpy>=1.14 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.23.5)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp>2) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sande\\appdata\\roaming\\python\\python38\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp>2) (3.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sande\\anaconda3\\lib\\site-packages (from requests-oauthlib->kubernetes<31,>=8.0.0->kfp>2) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sande\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\sande\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sande\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -tatsmodels (c:\\users\\sande\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7784286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581f8117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "ModuleNotFoundError: No module named 'kfp'\n",
      "'grep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "ModuleNotFoundError: No module named 'google_cloud_pipeline_components'\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b608be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "C:\\Users\\sande\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "C:\\Users\\sande\\AppData\\Local\\Temp\\ipykernel_12120\\1542185693.py:17: FutureWarning:  Google Cloud Pipeline Components will drop support for Python 3.8 on Oct 1, 2024. To use new versions of the GCPC SDK after that date, you will need to upgrade to Python >= 3.9. See https://devguide.python.org/versions/ for more details.\n",
      "  from google_cloud_pipeline_components.v1.model import ModelUploadOp\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,ModelDeployOp)\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331825b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"your project id\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"your url to pipeline root\"   # e.g., gs://temp_de2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af121d",
   "metadata": {},
   "source": [
    "## First create a function that trains a SVM model on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc9fec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_svm(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a SVM with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn import svm\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.read_csv(features.path)\n",
    "    df = df.drop(columns=['Id']) # have to drop the variable\n",
    "    \n",
    "    logging.info(df.columns)        \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.drop('Species',axis=1), \n",
    "                                                    df['Species'], test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "    model_svm = svm.SVC()\n",
    "    model_svm.fit(x_train,y_train)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": model_svm.score(x_test, y_test)\n",
    "    }\n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"svm\"\n",
    "   # Save the model\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(model_svm, f)   \n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e104459",
   "metadata": {},
   "source": [
    "## Then train a decision tree on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31dcb7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_dt(features: Input[Dataset], out_model: Output[Model]) -> NamedTuple('outputs', metrics=dict):\n",
    "    '''train a Decision Tree with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn import tree\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import pickle  \n",
    "       \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.read_csv(features.path)\n",
    "    df = df.drop(columns=['Id'])\n",
    "    \n",
    "    logging.info(df.columns)        \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.drop('Species',axis=1), \n",
    "                                                    df['Species'], test_size=0.20, \n",
    "                                                    random_state=42)\n",
    "    model_dt = tree.DecisionTreeClassifier()\n",
    "    model_dt.fit(x_train,y_train)\n",
    "\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": model_dt.score(x_test, y_test)\n",
    "    }\n",
    "    logging.info(metrics_dict)  \n",
    "    \n",
    "    out_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    out_model.metadata[\"algorithm\"] = \"dt\"\n",
    "   # Save the model\n",
    "    model_file = out_model.path + \".pkl\"\n",
    "    with open(model_file, 'wb') as f:  \n",
    "        pickle.dump(model_dt, f)   \n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdad3f",
   "metadata": {},
   "source": [
    "## Then we compare the two model's performances to decide which one to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35da0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def compare_model(svm_metrics: dict, dt_metrics: dict) -> str:\n",
    "    import logging\n",
    "    import json\n",
    "    import sys\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(svm_metrics)\n",
    "    logging.info(dt_metrics)\n",
    "    if svm_metrics.get(\"accuracy\") > dt_metrics.get(\"accuracy\"):\n",
    "        return \"SVM\"\n",
    "    else :\n",
    "        return \"DT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f2dc14",
   "metadata": {},
   "source": [
    "## Then we upload the model to google cloud storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "394054fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_model_to_gcs(project_id: str, model_repo: str, model: Input[Model], model_name: str):\n",
    "    '''upload model to gsc'''\n",
    "    from google.cloud import storage   \n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)    \n",
    "  \n",
    "    # upload the model to GCS\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(model_repo)\n",
    "    blob = bucket.blob('model.pkl')\n",
    "    source_file_name= model.path + '.pkl'\n",
    "   \n",
    "    blob.upload_from_filename(source_file_name)    \n",
    "    \n",
    "    print(f\"File {source_file_name} uploaded to {model_repo}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d3ba3",
   "metadata": {},
   "source": [
    "## Then we create the complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "41f2531c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"iris-predictor-training-pipeline-v2\")\n",
    "def pipeline(project_id: str, data_bucket: str, dataset_uri: str, model_repo: str, model_repo_uri: str):\n",
    "    \n",
    "    # load the dataset\n",
    "    dataset_op = kfp.dsl.importer(\n",
    "        artifact_uri=dataset_uri,\n",
    "        artifact_class=Dataset,\n",
    "        reimport=False,\n",
    "    )\n",
    "    \n",
    "    # svm model\n",
    "    training_svm_job_run_op = train_svm(\n",
    "        features=dataset_op.output\n",
    "    )\n",
    "    \n",
    "    # dt model\n",
    "    training_dt_job_run_op = train_dt(\n",
    "        features=dataset_op.output\n",
    "    )\n",
    "        \n",
    "    # compare models\n",
    "    comp_model_op = compare_model(svm_metrics=training_svm_job_run_op.outputs[\"metrics\"],\n",
    "                                       dt_metrics=training_dt_job_run_op.outputs[\"metrics\"]).after(training_svm_job_run_op, training_dt_job_run_op)  \n",
    "    \n",
    "    # This part of the code did not work, so right now, we just use the DT model, whether it was better or not\n",
    "#     with dsl.If(comp_model_op.output=='SVM'):\n",
    "#         final_model_op = training_svm_job_run_op\n",
    "#     with dsl.If(comp_model_op.output=='DT'):\n",
    "#         final_model_op = training_dt_job_run_op\n",
    "    \n",
    "    # upload model to storage\n",
    "    upload_model_to_gc_op = upload_model_to_gcs(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,\n",
    "        model=training_dt_job_run_op.outputs['out_model'],\n",
    "        model_name=comp_model_op.output\n",
    "    ) \n",
    "    \n",
    "    # dont know why we do this but is for deploying the model i believe\n",
    "    import_unmanaged_model_task = dsl.importer(\n",
    "            artifact_uri=model_repo_uri,\n",
    "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "            metadata={\n",
    "                \"containerSpec\": {\n",
    "                    \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\",  # see https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers  \n",
    "                },\n",
    "            },\n",
    "        ).after(upload_model_to_gc_op)  \n",
    "    \n",
    "    # upload model to model registry\n",
    "    model_upload_op = ModelUploadOp(\n",
    "            project=project_id,\n",
    "            display_name=f\"iris-prediction-model-{comp_model_op.output}\",\n",
    "            unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "        ).after(import_unmanaged_model_task) \n",
    "    \n",
    "    # create endpoint for predictions\n",
    "    create_endpoint_op = EndpointCreateOp(\n",
    "            project=project_id,\n",
    "            display_name=\"iris-prediction-service\",\n",
    "        ).after(model_upload_op) \n",
    "    \n",
    "    # deploy full model\n",
    "    model_deploy_op = ModelDeployOp(\n",
    "            model=model_upload_op.outputs[\"model\"],\n",
    "            endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "            deployed_model_display_name=f\"iris-prediction-model-{comp_model_op.output}\",\n",
    "            dedicated_resources_machine_type=\"n1-standard-2\",\n",
    "            dedicated_resources_min_replica_count=1,\n",
    "            dedicated_resources_max_replica_count=1,\n",
    "            traffic_split={\"0\": 100},\n",
    "        ).after(create_endpoint_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb72f8",
   "metadata": {},
   "source": [
    "## Then we compile the YAML file, which is used to run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "258f5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='iris_predictor_training_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d4b95",
   "metadata": {},
   "source": [
    "## Then we run the pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "105cc7e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12120\\1169696711.py\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# Prepare the pipeline job\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m job = aip.PipelineJob(\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mdisplay_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"iris-predictor\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0menable_caching\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, display_name, template_path, job_id, pipeline_root, parameter_values, input_artifacts, enable_caching, encryption_spec_key_name, labels, credentials, project, location, failure_policy)\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         self._parent = initializer.global_config.common_location_path(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\google\\cloud\\aiplatform\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, project, location, credentials, resource_name)\u001b[0m\n\u001b[0;32m   1232\u001b[0m             \u001b[0mresource_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m \u001b[0mfully\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mqualified\u001b[0m \u001b[0mresource\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mID\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1233\u001b[0m         \"\"\"\n\u001b[1;32m-> 1234\u001b[1;33m         _VertexAiResourceNounPlus.__init__(\n\u001b[0m\u001b[0;32m   1235\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m             \u001b[0mproject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\google\\cloud\\aiplatform\\base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, project, location, credentials, resource_name)\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproject\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlocation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcredentials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcredentials\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcredentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mappended_user_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\google\\cloud\\aiplatform\\initializer.py\u001b[0m in \u001b[0;36mcredentials\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mlogging_warning_filter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoggingFilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWARNING\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddFilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging_warning_filter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_project_as_env_var_or_google_auth_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m         \u001b[0mcredentials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_credentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremoveFilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogging_warning_filter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\google\\cloud\\aiplatform\\initializer.py\u001b[0m in \u001b[0;36m_set_project_as_env_var_or_google_auth_default\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_credentials\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_api_key\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m             \u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_credentials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\google\\auth\\_default.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meffective_project_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDefaultCredentialsError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_CLOUD_SDK_MISSING_CREDENTIALS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"iris-predictor\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"iris_predictor_training_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID, # makesure to use your project id \n",
    "        'data_bucket': 'data_de2024_2083033',  # makesure to use your data bucket name \n",
    "        'dataset_uri':'gs://data_de2024_2083033/Iris.csv',\n",
    "        'model_repo':'models_de2024_2083033', # makesure to use your model bucket name \n",
    "        'model_repo_uri':'gs://models_de2024_2083033' # makesure to use your model bucket name \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e443112c",
   "metadata": {},
   "source": [
    "## Small test to see if model is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f5392",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISPLAY_NAME = \"iris-prediction-model\"\n",
    "! gcloud ai models list --region={REGION} --filter={DISPLAY_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85ff070",
   "metadata": {},
   "source": [
    "## Create an endpoint prediction for a random instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b5bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_NAME=\"iris-prediction-service\"\n",
    "instance = [[6.7,3.3,5.7,2.5]]  # Prediciton request inputs \n",
    "ENDPOINT_ID = !(gcloud ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)'\\\n",
    "              --filter=display_name=$ENDPOINT_NAME \\\n",
    "              --sort-by=creationTimeStamp | tail -1)\n",
    "ENDPOINT_ID = ENDPOINT_ID[1]\n",
    "\n",
    "def endpoint_predict(\n",
    "    project: str, location: str, instances: list, endpoint: str\n",
    "):\n",
    "    aip.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aip.Endpoint(endpoint)\n",
    "\n",
    "    prediction = endpoint.predict(instances=instances)\n",
    "    return prediction\n",
    "\n",
    "endpoint_predict(PROJECT_ID, REGION, instance, ENDPOINT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb225b27",
   "metadata": {},
   "source": [
    "## Create a batch prediction for a random file with input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be9a998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables \n",
    "job_display_name = \"iris-prediction-batch-prediction-job\"\n",
    "MODEL_NAME=\"iris-prediction-model-DT\"\n",
    "ENDPOINT_NAME=\"iris-prediction-service\"\n",
    "BUCKET_URI=\"gs://data_de2024_2083033\"\n",
    "input_file_name=\"iris_batch.csv\"\n",
    "\n",
    "# Get model id\n",
    "MODEL_ID=!(gcloud ai models list --region=$REGION \\\n",
    "           --filter=display_name=$MODEL_NAME)\n",
    "print(MODEL_ID)\n",
    "MODEL_ID=MODEL_ID[2].split(\" \")[0]\n",
    "\n",
    "model_resource_name = f'projects/{PROJECT_ID}/locations/{REGION}/models/{MODEL_ID}'\n",
    "print(model_resource_name)\n",
    "gcs_source= [f\"{BUCKET_URI}/{input_file_name}\"]\n",
    "gcs_destination_prefix=f\"{BUCKET_URI}/output\"\n",
    "\n",
    "def batch_prediction_job(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_resource_name: str,\n",
    "    job_display_name: str,\n",
    "    gcs_source: str,\n",
    "    gcs_destination_prefix: str,\n",
    "    machine_type: str,\n",
    "    starting_replica_count: int = 1, # The number of nodes for this batch prediction job. \n",
    "    max_replica_count: int = 1,    \n",
    "):   \n",
    "    aip.init(project=project, location=location)\n",
    "\n",
    "    model = aip.Model(model_resource_name)\n",
    "\n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        instances_format='csv', #csv\n",
    "        gcs_source=[f\"{BUCKET_URI}/{input_file_name}\"],\n",
    "        gcs_destination_prefix=f\"{BUCKET_URI}/output\",\n",
    "        machine_type=machine_type, # must be present      \n",
    "    )\n",
    "    batch_prediction_job.wait()\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job\n",
    "\n",
    "batch_prediction_job(PROJECT_ID, REGION, model_resource_name, job_display_name, gcs_source, gcs_destination_prefix, machine_type=\"n1-standard-2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
